# -*- coding: utf-8 -*-
"""heading_error_ffnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZcG9XwE2EnnPeQRmYbcM3dWeERNSM_8i
"""

import pandas as pd
import numpy as np

df = pd.read_csv("heading_error_dataset.csv")

df['heading_error'] = df['heading_error']

# 2) delta normalization
df['delta_encL'] = df['delta_encL'] / 100.0
df['delta_encR'] = df['delta_encR'] / 100.0

# 3) label one-hot
df['label_forward'] = (df['label'] == 'forward').astype(int)
df['label_left']     = (df['label'] == 'left').astype(int)
df['label_right']    = (df['label'] == 'right').astype(int)

# 4) input col
feature_cols = [
    'label_forward', 'label_left', 'label_right',
    'delta_encL', 'delta_encR', 'heading_error'
]

# 5) output col
label_cols = ['cmdL', 'cmdR']

# 6) X(t), Y(t+1)
X = df[feature_cols].iloc[:-1].values
Y = df[label_cols].iloc[1:].values

print("X shape:", X.shape)
print("Y shape:", Y.shape)

print("X :", X[:5])
print("Y :", Y[:5])

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt

# Y /127
Y_norm = Y.astype(np.float32) / 127.0

# model structure
model = Sequential([
    Dense(64, activation='relu', input_shape=(X.shape[1],)),  # 첫 레이어 64
    Dense(32, activation='relu'),                             # 두 번째 레이어 32
    Dense(16, activation='relu'),                             # 세 번째 레이어 16
    Dense(2, activation='sigmoid')                            # 출력: curr_cmdL/R (0~1 스케일)
])


model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

#
history = model.fit(X, Y_norm, epochs=50, batch_size=64, validation_split=0.2)

# check results
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# predict
Y_pred = model.predict(X[:5])
print("실제 Y:\n", Y[:5])
print("예측 Y:\n", Y_pred)

pred_scaled = (Y_pred * 127.0).astype(np.int32)

#more predict
Y_pred = model.predict(X[:10])
pred_scaled = (Y_pred * 127.0).astype(np.int32)
print("실제 Y:", Y[:10],"예측 Y:\n", pred_scaled)



Y_pred = model.predict(X[3045:3050])
pred_scaled = (Y_pred * 127.0).astype(np.int32)
print("실제 Y:", Y[3045:3050],"예측 Y:\n", pred_scaled)

Y_pred = model.predict(X[1045:1050])
pred_scaled = (Y_pred * 127.0).astype(np.int32)
print("실제 Y:", Y[1045:1050],"예측 Y:\n", pred_scaled)

for layer in model.layers:
    w = layer.get_weights()
    if len(w) == 0:
        continue
    W, b = w
    print(f"\n=== Layer: {layer.name} ===")
    print("Weights shape:", W.shape)
    print(W)
    print("Bias shape:", b.shape)
    print(b)

#print weight & bias for HLS
layer_id = 1

for layer in model.layers:
    weights = layer.get_weights()

    # Dense 레이어만 처리 (weight가 없는 레이어는 skip)
    if len(weights) == 0:
        continue

    W, b = weights
    in_dim, out_dim = W.shape

    # ---- Weight 출력 ----
    print(f"// Layer {layer_id} Weights ({in_dim} x {out_dim})")
    print(f"const float W{layer_id}[{in_dim}][{out_dim}] = {{")
    for i in range(in_dim):
        row = ", ".join(f"{W[i][j]:.8f}" for j in range(out_dim))
        print(f"    {{ {row} }},")
    print("};\n")

    # ---- Bias 출력 ----
    print(f"// Layer {layer_id} Bias ({out_dim})")
    bias_line = ", ".join(f"{b[j]:.8f}" for j in range(out_dim))
    print(f"const float b{layer_id}[{out_dim}] = {{ {bias_line} }};\n")

    layer_id += 1

import numpy as np
import tensorflow as tf

# ---- Functional API로 layer-by-layer 모델 재구성 ----
inputs = tf.keras.Input(shape=(X.shape[1],))
x = inputs
outs = []

for layer in model.layers:
    x = layer(x)
    outs.append(x)

# 여러 레이어 출력 받는 모델
intermediate_model = tf.keras.Model(inputs=inputs, outputs=outs)

# ---- 테스트 샘플 1개 입력 ----
sample = X[1:2]  # 또는 아무 입력 1개

layer_outputs = intermediate_model.predict(sample)

# ---- 레이어별 출력 확인 ----
for i, out in enumerate(layer_outputs):
    print(f"\n===== Layer {i} ({model.layers[i].name}) Output =====")
    print(out)
    print("shape:", out.shape)

# ---- 마지막 레이어 pre-activation(z) 계산 ----
last_layer = model.layers[-1]          # 마지막 Dense
W, b = last_layer.get_weights()        # W: (in, out), b: (out,)

# 이전 레이어 출력 (activation 전)
prev_out = layer_outputs[-2]           # 마지막 레이어 직전 출력

# pre-activation 계산: z = Wx + b
z = np.dot(prev_out, W) + b
print("\n===== Last Layer Pre-activation (z) =====")
print(z)
print("shape:", z.shape)







model.save("my_model.keras")

