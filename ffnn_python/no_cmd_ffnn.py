# -*- coding: utf-8 -*-
"""no_cmd_ffnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WTLO4FxvgDb563ChIf2tTv4o8MdQi3nz
"""

import pandas as pd
import numpy as np

# CSV 읽기
df = pd.read_csv("dataset.csv")

# 1️⃣ heading 0~360 -> radian -> sin/cos
df['heading_rad'] = np.deg2rad(df['heading'])
df['sin_heading'] = np.sin(df['heading_rad'])
df['cos_heading'] = np.cos(df['heading_rad'])

# 2️⃣ delta 값 정규화
df['delta_encL'] = df['delta_encL'] / 100.0
df['delta_encR'] = df['delta_encR'] / 100.0

# 3️⃣ 강제로 3개 클래스 원-핫 라벨 (직진만 1, 좌/우 0)
df['label_forward'] = 1
df['label_left'] = 0
df['label_right'] = 0
label_cols = ['label_forward', 'label_left', 'label_right']  # 가장 앞

df['cmdL_scaled'] = df['cmdL'] / 12.70
df['cmdR_scaled'] = df['cmdR'] / 12.70



# 4️⃣ 윈도우 길이
window_size = 5


label_cols = ['label_forward', 'label_left', 'label_right']
feature_cols = ['delta_encL', 'delta_encR', 'sin_heading', 'cos_heading']

X, Y = [], []

for i in range(len(df) - window_size):
    window = []

    # 0~4번째 timestep을 순서대로 넣기
    for t in range(window_size):
        row = df.iloc[i + t]

        # [label..., feature...] 순서로 붙이기
        window.extend(row[label_cols].values.tolist())
        window.extend(row[feature_cols].values.tolist())

    X.append(window)

    # 출력: 다음 timestep의 cmdL, cmdR
    Y.append(df.iloc[i + window_size][['cmdL', 'cmdR']].values)

X = np.array(X, dtype=np.float32)
Y = np.array(Y, dtype=np.float32)

print("입력 X shape:", X.shape)  # (샘플 수, window_size * (label+feature))
print("출력 Y shape:", Y.shape)  # (샘플 수, 2)

print("입력 5개:\n", X[:5])

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import numpy as np
import matplotlib.pyplot as plt

# X, Y: 이전 단계에서 만든 numpy 배열
Y_norm = Y.astype(np.float32) / 127.0

# 1️⃣ 하드웨어 친화적 FFNN 모델 정의
model = Sequential([
    Dense(64, activation='relu', input_shape=(X.shape[1],)),  # 첫 레이어 64
    Dense(32, activation='relu'),                             # 두 번째 레이어 32
    Dense(16, activation='relu'),                             # 세 번째 레이어 16
    Dense(2, activation='sigmoid')                            # 출력: curr_cmdL/R (0~1 스케일)
])

# 2️⃣ 모델 컴파일
model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# 3️⃣ 모델 학습
history = model.fit(X, Y_norm, epochs=60, batch_size=64, validation_split=0.2)

# 4️⃣ 학습 결과 확인
plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# 5️⃣ 예측 예시
Y_pred = model.predict(X[:5])
print("실제 Y:\n", Y[:5])
print("예측 Y:\n", Y_pred)

pred_scaled = (Y_pred * 127.0).astype(np.int32)

Y_pred = model.predict(X[:10])
pred_scaled = (Y_pred * 127.0).astype(np.int32)
print("실제 Y:", Y[:10],"예측 Y:\n", pred_scaled)



Y_pred = model.predict(X[3045:3050])
pred_scaled = (Y_pred * 127.0).astype(np.int32)
print("실제 Y:", Y[3045:3050],"예측 Y:\n", pred_scaled)

Y_pred = model.predict(X[1045:1050])
pred_scaled = (Y_pred * 127.0).astype(np.int32)
print("실제 Y:", Y[1045:1050],"예측 Y:\n", pred_scaled)

for layer in model.layers:
    w = layer.get_weights()
    if len(w) == 0:
        continue
    W, b = w
    print(f"\n=== Layer: {layer.name} ===")
    print("Weights shape:", W.shape)
    print(W)
    print("Bias shape:", b.shape)
    print(b)

layer_id = 1

for layer in model.layers:
    weights = layer.get_weights()

    # Dense 레이어만 처리 (weight가 없는 레이어는 skip)
    if len(weights) == 0:
        continue

    W, b = weights
    in_dim, out_dim = W.shape

    # ---- Weight 출력 ----
    print(f"// Layer {layer_id} Weights ({in_dim} x {out_dim})")
    print(f"const fixed16_2 W{layer_id}[{in_dim}][{out_dim}] = {{")
    for i in range(in_dim):
        row = ", ".join(f"{W[i][j]:.8f}" for j in range(out_dim))
        print(f"    {{ {row} }},")
    print("};\n")

    # ---- Bias 출력 ----
    print(f"// Layer {layer_id} Bias ({out_dim})")
    bias_line = ", ".join(f"{b[j]:.8f}" for j in range(out_dim))
    print(f"const fixed16_2 b{layer_id}[{out_dim}] = {{ {bias_line} }};\n")

    layer_id += 1

import numpy as np
import tensorflow as tf

# ---- Functional API로 layer-by-layer 모델 재구성 ----
inputs = tf.keras.Input(shape=(X.shape[1],))
x = inputs
outs = []

for layer in model.layers:
    x = layer(x)
    outs.append(x)

# 여러 레이어 출력 받는 모델
intermediate_model = tf.keras.Model(inputs=inputs, outputs=outs)

# ---- 테스트 샘플 1개 입력 ----
sample = X[1:2]  # 또는 아무 입력 1개

layer_outputs = intermediate_model.predict(sample)

# ---- 레이어별 출력 확인 ----
for i, out in enumerate(layer_outputs):
    print(f"\n===== Layer {i} ({model.layers[i].name}) Output =====")
    print(out)
    print("shape:", out.shape)

# ---- 마지막 레이어 pre-activation(z) 계산 ----
last_layer = model.layers[-1]          # 마지막 Dense
W, b = last_layer.get_weights()        # W: (in, out), b: (out,)

# 이전 레이어 출력 (activation 전)
prev_out = layer_outputs[-2]           # 마지막 레이어 직전 출력

# pre-activation 계산: z = Wx + b
z = np.dot(prev_out, W) + b
print("\n===== Last Layer Pre-activation (z) =====")
print(z)
print("shape:", z.shape)







model.save("my_model.keras")

